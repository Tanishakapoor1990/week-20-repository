{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fa2550a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lorem', 'ipsum', 'dolor', 'sit', 'amet', ',', 'consectetur', 'adipiscing', 'elit', '.', 'Suspendisse', 'dignissim', 'pretium', 'lectus', ',', 'dignissim', 'molestie', 'lacus', 'viverra', 'eu', '.', 'Sed', 'venenatis', 'mauris', 'vel', 'tellus', 'ornare', 'tempus', '.', 'Aenean', 'in', 'laoreet', 'eros', '.', 'Duis', 'feugiat', ',', 'elit', 'non', 'maximus', 'auctor', ',', 'sapien', 'ex', 'vehicula', 'nulla', ',', 'sit', 'amet', 'cursus', 'dui', 'orci', 'ut', 'tellus', '.', 'Duis', 'cursus', 'volutpat', 'convallis', '.', 'Maecenas', 'posuere', 'tortor', 'enim', '.', 'Maecenas', 'vulputate', 'mauris', 'placerat', ',', 'ultrices', 'odio', 'in', ',', 'condimentum', 'arcu', '.', 'Cras', 'sollicitudin', 'mauris', 'at', 'justo', 'tristique', ',', 'at', 'condimentum', 'ante', 'ornare', '.', 'Vivamus', 'cursus', 'sapien', 'et', 'sapien', 'mattis', 'tristique', '.', 'Duis', 'et', 'tincidunt', 'ipsum', '.', 'Integer', 'interdum', 'vulputate', 'lorem', ',', 'sed', 'hendrerit', 'felis', 'pulvinar', 'ut', '.', 'Vivamus', 'non', 'lectus', 'bibendum', 'imperdiet', 'pretium', 'vel', 'in', 'libero', '.', 'Aliquam', 'semper', 'tincidunt', 'turpis', 'et', 'feugiat', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "#https://www.lipsum.com/feed/html\n",
    "text='''Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n",
    "Suspendisse dignissim pretium lectus, \n",
    "dignissim molestie lacus viverra eu. \n",
    "Sed venenatis mauris vel tellus ornare tempus. \n",
    "Aenean in laoreet eros. Duis feugiat, elit non maximus\n",
    "auctor, sapien ex vehicula nulla, sit amet cursus dui \n",
    "orci ut tellus. Duis cursus volutpat convallis. \n",
    "Maecenas posuere tortor enim. Maecenas vulputate \n",
    "mauris placerat, ultrices odio in, condimentum arcu. \n",
    "Cras sollicitudin mauris at justo tristique, at \n",
    "condimentum ante ornare. Vivamus cursus sapien et \n",
    "sapien mattis tristique. Duis et tincidunt ipsum. \n",
    "Integer interdum vulputate lorem, sed hendrerit \n",
    "felis pulvinar ut. Vivamus non lectus bibendum \n",
    "imperdiet pretium vel in libero. Aliquam semper \n",
    "tincidunt turpis et feugiat.'''\n",
    "print(word_tokenize(text))\n",
    "#keras has a tokenzier\n",
    "#boundary of words can be complicated, but in english this is not as hard as some other langauges\n",
    "#handling symbols can also be hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83b42ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('gutenberg')\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1527c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiw = nltk.corpus.gutenberg.words('carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86d29111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Alice', \"'\", 's', 'Adventures', 'in', ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aiw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f80796d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sunny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\sunny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')\n",
    "#nltk.corpus.gutenberg.fields()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d3bc828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Alice', \"'\", 's', 'Adventures', 'in', ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alw = nltk.corpus.gutenberg.words('carroll-alice.txt')\n",
    "alw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30af6211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words: ['them', 'hasn', 'off', \"wouldn't\", 'wouldn', 'from', 'or', 'an', \"isn't\", 'out']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sunny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# stop words are common words that have low information value in text\n",
    "#we generally get rid of stop words\n",
    "nltk.download('stopwords')\n",
    "sw = set(nltk.corpus.stopwords.words('english'))\n",
    "print(\"Stop words: \" + str(list(sw)[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fdcd41",
   "metadata": {},
   "source": [
    "## Stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a2a8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop words are common words that have low information value in txt \n",
    "#we generally get rid of these words\n",
    "#they are pronouns and they are all lower case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58efabf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words: ['them', 'hasn', 'off', \"wouldn't\", 'wouldn', 'from', 'or', 'an', \"isn't\", 'out']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sunny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "sw = set(nltk.corpus.stopwords.words('english'))\n",
    "print(\"Stop words: \" + str(list(sw)[:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20cc79d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[', 'Alice', \"'\", 's', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865', ']'], ['CHAPTER', 'I', '.'], ['Down', 'the', 'Rabbit', '-', 'Hole'], ['Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'having', 'nothing', 'to', 'do', ':', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', ',', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', ',', \"'\", 'and', 'what', 'is', 'the', 'use', 'of', 'a', 'book', \",'\", 'thought', 'Alice', \"'\", 'without', 'pictures', 'or', 'conversation', \"?'\"], ['So', 'she', 'was', 'considering', 'in', 'her', 'own', 'mind', '(', 'as', 'well', 'as', 'she', 'could', ',', 'for', 'the', 'hot', 'day', 'made', 'her', 'feel', 'very', 'sleepy', 'and', 'stupid', '),', 'whether', 'the', 'pleasure', 'of', 'making', 'a', 'daisy', '-', 'chain', 'would', 'be', 'worth', 'the', 'trouble', 'of', 'getting', 'up', 'and', 'picking', 'the', 'daisies', ',', 'when', 'suddenly', 'a', 'White', 'Rabbit', 'with', 'pink', 'eyes', 'ran', 'close', 'by', 'her', '.'], ['There', 'was', 'nothing', 'so', 'VERY', 'remarkable', 'in', 'that', ';', 'nor', 'did', 'Alice', 'think', 'it', 'so', 'VERY', 'much', 'out', 'of', 'the', 'way', 'to', 'hear', 'the', 'Rabbit', 'say', 'to', 'itself', ',', \"'\", 'Oh', 'dear', '!'], ['Oh', 'dear', '!'], ['I', 'shall', 'be', 'late', \"!'\"], ['(', 'when', 'she', 'thought', 'it', 'over', 'afterwards', ',', 'it', 'occurred', 'to', 'her', 'that', 'she', 'ought', 'to', 'have', 'wondered', 'at', 'this', ',', 'but', 'at', 'the', 'time', 'it', 'all', 'seemed', 'quite', 'natural', ');', 'but', 'when', 'the', 'Rabbit', 'actually', 'TOOK', 'A', 'WATCH', 'OUT', 'OF', 'ITS', 'WAISTCOAT', '-', 'POCKET', ',', 'and', 'looked', 'at', 'it', ',', 'and', 'then', 'hurried', 'on', ',', 'Alice', 'started', 'to', 'her', 'feet', ',', 'for', 'it', 'flashed', 'across', 'her', 'mind', 'that', 'she', 'had', 'never', 'before', 'seen', 'a', 'rabbit', 'with', 'either', 'a', 'waistcoat', '-', 'pocket', ',', 'or', 'a', 'watch', 'to', 'take', 'out', 'of', 'it', ',', 'and', 'burning', 'with', 'curiosity', ',', 'she', 'ran', 'across', 'the', 'field', 'after', 'it', ',', 'and', 'fortunately', 'was', 'just', 'in', 'time', 'to', 'see', 'it', 'pop', 'down', 'a', 'large', 'rabbit', '-', 'hole', 'under', 'the', 'hedge', '.'], ['In', 'another', 'moment', 'down', 'went', 'Alice', 'after', 'it', ',', 'never', 'once', 'considering', 'how', 'in', 'the', 'world', 'she', 'was', 'to', 'get', 'out', 'again', '.']]\n"
     ]
    }
   ],
   "source": [
    "text_sentences = nltk.corpus.gutenberg.sents(\"carroll-alice.txt\")[:10]\n",
    "print(text_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b2aa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "echo \"the quick brown fox jumps over the lazy dog\"|./mapper.py|sort|./reducer.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "454ccbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Alice', \"'\", 'Adventures', 'Wonderland', 'Lewis', 'Carroll', '1865', ']']\n",
      "['CHAPTER', '.']\n",
      "['Rabbit', '-', 'Hole']\n",
      "['Alice', 'beginning', 'get', 'tired', 'sitting', 'sister', 'bank', ',', 'nothing', ':', 'twice', 'peeped', 'book', 'sister', 'reading', ',', 'pictures', 'conversations', ',', \"'\", 'use', 'book', \",'\", 'thought', 'Alice', \"'\", 'without', 'pictures', 'conversation', \"?'\"]\n",
      "['considering', 'mind', '(', 'well', 'could', ',', 'hot', 'day', 'made', 'feel', 'sleepy', 'stupid', '),', 'whether', 'pleasure', 'making', 'daisy', '-', 'chain', 'would', 'worth', 'trouble', 'getting', 'picking', 'daisies', ',', 'suddenly', 'White', 'Rabbit', 'pink', 'eyes', 'ran', 'close', '.']\n",
      "['nothing', 'remarkable', ';', 'Alice', 'think', 'much', 'way', 'hear', 'Rabbit', 'say', ',', \"'\", 'Oh', 'dear', '!']\n",
      "['Oh', 'dear', '!']\n",
      "['shall', 'late', \"!'\"]\n",
      "['(', 'thought', 'afterwards', ',', 'occurred', 'ought', 'wondered', ',', 'time', 'seemed', 'quite', 'natural', ');', 'Rabbit', 'actually', 'TOOK', 'WATCH', 'WAISTCOAT', '-', 'POCKET', ',', 'looked', ',', 'hurried', ',', 'Alice', 'started', 'feet', ',', 'flashed', 'across', 'mind', 'never', 'seen', 'rabbit', 'either', 'waistcoat', '-', 'pocket', ',', 'watch', 'take', ',', 'burning', 'curiosity', ',', 'ran', 'across', 'field', ',', 'fortunately', 'time', 'see', 'pop', 'large', 'rabbit', '-', 'hole', 'hedge', '.']\n",
      "['another', 'moment', 'went', 'Alice', ',', 'never', 'considering', 'world', 'get', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in text_sentences:\n",
    "    filtered_list = [w for w in sentence if w.lower() not in sw]\n",
    "    print(filtered_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52133ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming \n",
    "finding root words \n",
    "over stemmming - words are over truncated\n",
    "understemming (FN)\n",
    "from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9866589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(sentences):\n",
    "    total_list=[]\n",
    "    sentence_list=[]\n",
    "    for sentence in sentences:\n",
    "        #print(sentence)\n",
    "        sentence_list.append(sentence)\n",
    "        \n",
    "        for word in sentence:\n",
    "            #print(word)\n",
    "            new_sentence = ' '.join(word)\n",
    "            #print(new_sentence)\n",
    "        total_list.append(new_sentence)   \n",
    "    return total_list\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
